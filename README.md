# FlipSide

**The dark side of small print.**

> What did you agree to? Upload a document you didn't write. See what the other side intended.

---

## The Problem

Every day, millions of people accept documents they did not draft and do not fully understand. Not just contracts — sweepstakes rules, coupon booklets, gym memberships, pet adoption papers, wedding venue agreements, timeshare packages, insurance policies, app Terms of Service.

These documents are written by one party's legal team, marketing department, or compliance office to protect *that party's* interests. The person signing — or clicking "I Agree" — sees the words. They do not see the strategy behind the words.

There is no tool that shows you what a document looks like from the other side — from the perspective of the party who drafted it.

---

## What FlipSide Does

You upload any document someone else wrote for you to sign, accept, or agree to. FlipSide reads it as if it were the drafter's attorney — the person who wrote it and knows exactly why every clause is there. Then it tells you what it found.

**One input. One perspective flip. One output.**

### Try It Instantly — 14 Built-In Samples

No document handy? Pick a life moment:

| Your moment | The document | What FlipSide finds |
| --- | --- | --- |
| <img src="static/tile_lease.jpg" width="20"> **My New Place** | 12-month residential lease | Late fees that cascade into unpayable debt |
| <img src="static/tile_insurance.jpg" width="20"> **Peace of Mind** | Homeowner's coverage plan | Two exclusion clauses that interact to deny most real-world water damage claims |
| <img src="static/tile_gym.jpg" width="20"> **Morning Workout** | 24-month fitness membership | Auto-renewal kicks in silently; cancellation window is 30 days before a date they never told you |
| <img src="static/tile_coupon.jpg" width="20"> **Weekend Savings** | Local store rewards program | "Savings" require minimum spend thresholds that make every "deal" cost more than buying at full price |
| <img src="static/tile_employment.jpg" width="20"> **First Day of Work** | Tech company offer letter | An at-will clause renders every other promise (discipline process, severance) unenforceable |
| <img src="static/tile_loan.jpg" width="20"> **Future Fund** | Personal auto loan terms | A single late payment triggers a cascade of penalties designed to stack |
| <img src="static/tile_tos.jpg" width="20"> **Staying Connected** | Social media platform terms | "We may share with partners" means unrestricted sale to data brokers |
| <img src="static/tile_medical.jpg" width="20"> **Check-up Time** | Outpatient procedure consent | A liability waiver placed after medical disclosures to benefit from the assumption that everything on the form is standard |
| <img src="static/tile_hoa.jpg" width="20"> **Dream Home** | Suburban community bylaws | The board can levy special assessments with no cap and fine you daily for violations defined at their sole discretion |
| <img src="static/tile_wedding.jpg" width="20"> **Celebration!** | Event venue rental agreement | Force majeure protects the venue but not you — if they cancel, you get a credit; if you cancel, you lose everything |
| <img src="static/tile_pet.jpg" width="20"> **Furry Friend** | Shelter adoption contract | A return clause lets the shelter reclaim the animal at any time if they judge your care "inadequate" — no appeal |
| <img src="static/tile_timeshare.jpg" width="20"> **Dream Getaway** | Timeshare vacation package | The "cooling off" period is shorter than payment processing time, making the exit window structurally impossible to use |
| <img src="static/thumb_sweepstakes.jpg" width="20"> **Trip of a Lifetime** | Sweepstakes official rules (real Coca-Cola) | Accepting the prize grants perpetual, worldwide rights to your name, likeness, and story — with no compensation |
| <img src="static/tile_hackathon.jpg" width="20"> **The Hackathon** | Event waiver we all signed (real) | You grant a perpetual, irrevocable license to all your Materials — code, ideas, feedback — with no confidentiality obligation |

Plus: **paste text directly**, **paste a URL**, **compare two documents** side by side, or upload your own PDF, DOCX, or TXT.

---

### The Four Steps

1. **Upload** — Drag in a PDF, DOCX, paste text, paste a URL, or pick a sample. FlipSide handles everything from a one-page gym contract to a 40-page insurance policy.

2. **Browse flip cards** — The first card appears in **~8 seconds** (streaming pipeline — see Architecture). Each card is a clause with two sides:

   * **Front**: A calm green header with a reassurance headline ("Your flexible payment timeline") followed by the reader's gullible first impression. This is how the drafter WANTS you to feel. Navigation is hidden until you flip your first card — forcing the core mechanic.
   * **Back**: What the drafter intended — risk score, trick classification, the key figure in bold ("$4,100 in penalties"), a concrete scenario, and bottom-line action. The sidebar dims to 35% opacity to spotlight the reveal.
   * **Confidence badge**: HIGH / MEDIUM / LOW with reasoned explanation
   * **Color-coded**: Green (genuinely fair) · Yellow (notable) · Red (strategically asymmetric)
   * **Document preview**: Sidebar shows the full text with numbered clause markers (①②③) that highlight as you navigate cards
   * All output in English regardless of document language. Download the full report translated to the document's original language.

3. **Read the Expert Verdict** — While you browse cards, Opus 4.6 builds a one-screen verdict covering:

   * **Verdict Tier** — one of five levels from "Sign with Confidence" to "Do Not Sign"
   * **The Main Thing** — the single worst risk with concrete consequences and clause references
   * **Power Ratio** — their rights vs. your rights, counted from the document
   * **Jurisdiction** — auto-detected with local law violations flagged (illegal vs. merely unfair)
   * **Risks & Checklist** — additional risks + chronological action items
   * If cards haven't arrived after 10 seconds, the verdict **auto-reveals** so you're never staring at a blank screen

4. **Take action** — After the verdict:

   * **Ask follow-up questions** — "What happens if I'm 3 months late on rent?" / "Which clauses can I negotiate?" — Opus traces the answer through all relevant clauses
   * **Message the company** — One-click draft of a professional letter citing the specific high-risk clauses, ready to copy or email
   * **Counter-draft** — Opus rewrites unfair clauses with negotiable alternatives you can propose
   * **Download report** — Export the full analysis, optionally translated to the document's original language

---

## Who Uses It

FlipSide works on any document someone else wrote for you to sign, accept, or agree to:

| User | Document | What they learn |
| --- | --- | --- |
| **Tenant** | Lease agreement | "Repairs not covered by building insurance" means they pay for plumbing, electrical, and HVAC |
| **Homeowner** | Insurance policy | Two exclusion clauses interact to deny most real-world water damage claims |
| **Employee** | Employment contract | An at-will clause renders every other promise unenforceable |
| **App user** | Terms of Service | "We may share with partners" means unrestricted sale to data brokers |
| **Borrower** | Loan agreement | A single late payment triggers a cascade of penalties designed to stack |
| **Gym member** | Fitness membership | Auto-renewal kicks in silently, cancellation window is structurally impossible |
| **Patient** | Medical consent form | A liability waiver benefits from the assumption that everything on the form is standard |
| **Homeowner** | HOA bylaws | The board can fine you daily for violations defined at their sole discretion |
| **Shopper** | Coupon reward program | Every "deal" costs more than buying at full price |
| **Couple** | Wedding venue contract | Force majeure protects the venue but not you |
| **Pet owner** | Adoption contract | The shelter can reclaim the animal at any time — no appeal |
| **Vacationer** | Timeshare package | The exit window is shorter than payment processing time |
| **Winner** | Sweepstakes rules | Accepting the prize grants perpetual rights to your name and likeness |
| **Hackathon participant** | Event waiver | All your Materials — code, ideas, feedback — licensed perpetually with no confidentiality |

---

## Opus 4.6 Capabilities — Used and Unused

FlipSide uses more Opus 4.6 capabilities than any single feature would require — including three that Anthropic [specifically highlights in the Opus 4.6 announcement](https://www.anthropic.com/news/claude-opus-4-6): adaptive thinking, long-context retrieval, and low over-refusals. Each used capability is **visible in the product** — not a behind-the-scenes optimization. We also document the features we considered but didn't use, and why.

| # | Opus 4.6 Feature | Used? | What the user does | What Opus 4.6 does |
| --- | --- | --- | --- | --- |
| 1 | **Adaptive thinking** | Yes | Uploads a document and sees the Opus verdict stream in ~30–60s. The model spends more reasoning on complex cross-clause interactions and less on straightforward sections. | The Opus verdict thread runs with `thinking: {type: 'adaptive'}`, meaning the model autonomously decides how many reasoning tokens to spend. A standard document gets a quick pass; a document with hidden cross-clause traps gets deep chain-of-thought. The user never configures this — Opus allocates its own thinking budget. |
| 2 | **Long-context retrieval** | Yes | Uploads a long document (tested up to 222 pages / ~167K tokens). Sees cross-clause interactions flagged in the verdict — e.g., "Clause 3 + Clause 297 create a compound trap." | The full document text is sent to the Opus verdict thread (`build_verdict_prompt`). The verdict prompt instructs cross-clause reasoning across the entire document. Opus must hold the entire document in context and connect clauses that may be hundreds of pages apart. Tested with 9 synthetic documents up to 300 clauses — 36/36 planted traps caught including §3↔§297 (distance 294 clauses). |
| 3 | **Low over-refusals** | Yes | Reads adversarial role-play in the verdict where the analysis adopts the drafter's perspective. The gullible reader voice on card fronts is deliberately naive; the card backs reveal what the drafter intended. Example: *"The math does the work. Two weeks late once and you'll never catch up."* | The card scan prompt creates a gullible reader who trusts the drafter completely. The verdict prompt and on-demand "Hidden Combinations" deep dive adopt the drafter's voice. Previous models would self-censor, add disclaimers, or refuse. Opus 4.6 fully commits to the perspective flip — the core product mechanic. |
| 4 | **Depth presets** | Yes | Chooses Quick / Standard / Deep analysis depth before uploading. Quick produces shorter output; Deep produces exhaustive analysis. | Three presets control `max_tokens` per API call: Quick (16K), Standard (32K), Deep (64K). This gives Opus more or less room to work. *Note: Architecture is ready to map these to Opus 4.6's native `effort` parameter (Quick→medium, Standard→high, Deep→max) when SDK support lands — currently a one-line change away.* |
| 5 | **Follow-up Q&A** | Yes | After analysis, types questions like "What happens if I'm 3 months late on rent?" or "Which clauses can I negotiate?" and gets traced answers. | Each follow-up sends the full document text + the user's question to Opus with adaptive thinking. Opus traces the answer through all relevant clauses in a single-turn exchange. Each question is independent — the full document is re-sent each time rather than accumulating conversation history. |
| 6 | **Vision / multimodal** | Yes | Uploads a PDF. Sees findings about visual tricks — fine print, buried placement, dense tables, light-gray disclaimers — that text extraction alone would miss. | PDF pages are rendered as JPEG images (150 DPI, up to 10 pages, max 4MB each) and sent as multimodal content blocks alongside the extracted text to the Opus verdict thread. The prompt includes: "Page images are included. Look for visual tricks: fine print, buried placement, dense tables, light-gray disclaimers." Opus processes both the text and the visual layout simultaneously. |
| 7 | **Structured output via prompts** | Yes | Sees consistently structured flip cards with risk scores, trick types (from a taxonomy of 18), confidence levels, figures, and examples — every clause in the same format. | Rather than using formal tool definitions, FlipSide achieves structured output through detailed prompt engineering. The card scan prompt specifies an exact output format with tagged fields (`[REASSURANCE]`, `[REVEAL]`, `Score:`, `Trick:`, etc.) and a constrained vocabulary of 18 trick types. The frontend parses these fields with regex. *The original plan included `assess_risk` and `flag_interaction` tool schemas, but prompt-based structuring proved more reliable for streaming.* |
| 8 | **Confidence calibration** | Yes | Each clause includes a Confidence level (HIGH / MEDIUM / LOW) with a one-line reasoning chain, generated by the model per defined criteria. | The card scan prompt instructs: "Confidence: HIGH = clear language, MEDIUM = some ambiguity, LOW = multiple interpretations." The model outputs `Confidence: [level] — [reason]` per clause. The Opus verdict thread produces a quality check with adjusted confidence after its self-review. The frontend parses these values; CSS for color-coded confidence badges is defined but the rendering step is not yet wired up — the data is captured for future display. |
| 9 | **Self-correction** | Yes | Reads the "Quality Check" in the verdict's colophon — where the analysis reviews itself for false positives and blind spots before the user acts on it. | The Opus verdict prompt (`build_verdict_prompt`) includes a dedicated `[COLOPHON]` section with methodology notes and self-review. The prompt instructs: "Be genuinely self-critical. Users trust uncertainty over false certainty." This runs as part of Opus's single output pass for the verdict. |
| 10 | **Split-model parallel** | Yes | First flip card appears in ~8 seconds; Opus verdict streams from t=0 and auto-reveals at 10s if cards haven't arrived. Cards trickle in as each parallel worker finishes. | Two-model pipeline: Haiku 4.5 pre-scans the document during upload to identify clauses, then N parallel Haiku workers build full flip cards (front + back) simultaneously. A single Opus 4.6 thread builds the one-screen verdict from t=0 in parallel. Cards emit in completion order (fastest first). The SSE stream is never blocked — Opus events flow from t=0. Haiku for instant structured cards, Opus for cross-clause reasoning, jurisdiction detection, and self-correction. |
| 11 | **Prompt caching** | Yes | Analyzes multiple documents without paying full price each time. The system prompts (which are identical across runs) are cached by Anthropic's API. | Every API call uses `cache_control: {type: 'ephemeral'}` on the system prompt, activating Anthropic's prompt caching (5-minute TTL). Cached input tokens cost 90% less than uncached. The actual per-run savings depend on the ratio of system prompt tokens (cached) to document tokens (not cached) — significant for repeated analyses but less than 90% of total cost. |
| 12 | **Counterfactual generation** | Yes | Sees the power ratio in the verdict (their rights vs. your rights) and can trigger an on-demand Counter-Draft to get rewritten clauses. | The verdict's `[POWER_RATIO]` tag compares the document's terms against a fair standard. The **Counter-Draft** (on-demand "Go Deeper" button): Opus rewrites each YELLOW/RED clause with fair alternatives, preserving the drafter's legitimate interests while removing exploitation. The prompt instructs: "Only redraft clauses that are genuinely unfair." |
| 13 | **Stylistic deduction** | Yes | The verdict identifies the drafter type and document patterns. The on-demand "Document Archaeology" deep dive classifies each section as boilerplate vs. custom-drafted. | The verdict prompt detects drafter patterns. The dedicated archaeology prompt (`build_archaeology_prompt`, available as an on-demand "Go Deeper" button) classifies each major section as **Boilerplate** or **Custom**, then profiles the drafter type. Example output: "This lease pattern is typical of high-volume property management companies optimizing for automated enforcement." Custom clauses are the signal — they reveal what the drafter cared enough to modify. |
| 14 | **English-only + download in language** | Yes | Reads the entire analysis in English regardless of document language. For non-English documents, sees a "Report in [language]" button that downloads the full analysis translated to the original language. | Every prompt includes `## LANGUAGE RULE: ALWAYS respond in ENGLISH regardless of the document's language.` Quotes are kept in the original language with English translations in parentheses. The frontend detects the document language from Haiku's output, shows a download button for non-English docs, then streams a full translation through Opus and generates a downloadable HTML report. Tested on West Frisian (~500K speakers) — correctly identified language, translated inline, cited Dutch BW articles. |
| 15 | **Effort controls** | No | — | Four explicit effort levels (low, medium, high, max) let the developer dial reasoning depth per API call. Our depth selector (Quick/Standard/Deep) currently maps to `max_tokens` instead. The Python SDK didn't expose the `effort` parameter during the hackathon. Architecture is ready — one-line change per preset when SDK support lands. |
| 16 | **Context compaction** | No | — | Automatically summarizes and replaces older context so the model can handle longer-running tasks without hitting the context window. Each FlipSide follow-up re-sends the full document as a fresh single-turn call — no conversation history accumulates, so there's nothing to compact. Would become useful if we added multi-turn follow-up sessions. |
| 17 | **Tool use** | No | — | Structured function calling where the model invokes developer-defined tools and receives results. We planned `assess_risk` and `flag_interaction` tools but found that prompt-based output formatting (tagged fields + regex parsing) was more reliable for streaming incremental cards. Infrastructure exists in the code but no tools are defined. |
| 18 | **Memory (beta)** | No | — | Persistent memory across conversations — the model remembers user preferences and prior interactions. FlipSide is stateless by design: upload → analyze → done. No user accounts, no session persistence, no need to remember across runs. |
| 19 | **Citations** | No | — | Model can cite specific passages from source documents in its output. We built our own citation system: numbered clause markers (①②③) in the sidebar that link to card positions, plus verbatim quotes in the prompt output format. Native citations could replace our regex-based quote extraction, but our visual marker system would still need custom rendering. |
| 20 | **Computer use** | No | — | Model can interact with desktop applications, browsers, and GUIs. FlipSide is an API-driven web app — no desktop interactions to automate. Not applicable to this product. |

### Why Opus 4.6 Specifically

Three capabilities from [Anthropic's Opus 4.6 announcement](https://www.anthropic.com/news/claude-opus-4-6) are **structurally necessary** for FlipSide to work. Remove any one and the product degrades:

1. **Adaptive thinking — the reasoning IS the product.** Opus 4.6 introduces four effort levels (low, medium, high, max) where the model picks up on contextual clues about how much extended thinking to use. The Opus verdict thread decides its own reasoning depth: spending more thinking tokens on complex cross-clause interactions and less on standard boilerplate. Most projects use extended thinking as a black box. FlipSide makes it visible in the expert verdict — the user sees the difference when Opus decides a document deserves deep reasoning.

2. **Long-context retrieval — finding legal traps spread across pages.** Opus 4.6 is the first Opus-class model with a **1M token context window** (beta). On the MRCR v2 8-needle 1M benchmark, it scores **76%** vs. Sonnet 4.5's **18.5%** — a 4× improvement that Anthropic calls "a qualitative shift in how much context a model can actually use." FlipSide applies this to cross-clause interaction detection: Clause 2(c) excludes water damage from "gradual seepage over 14 days." Clause 2(e) defines the inspection timeline at 30 days. Neither clause is dangerous alone. Together, they deny virtually all residential water damage claims. Tested to 222 pages (~167K tokens): 36/36 planted traps caught, including §3↔§297 (distance 294 clauses).

3. **Low over-refusals — the perspective flip works.** Anthropic reports Opus 4.6 has the **lowest rate of over-refusals among all recent Claude models** while maintaining an overall safety profile "as good as, or better than, any other frontier model." The gullible reader voice on card fronts is deliberately naive and trusting. Card backs reveal what the drafter actually intended. The on-demand "Hidden Combinations" deep dive adopts the drafter's adversarial voice. Previous models would self-censor, add disclaimers, or refuse entirely. Opus 4.6 fully commits to the perspective flip — the core product mechanic. This is not a safety bypass — it's a safety-conscious model that understands the educational purpose of perspective-taking.

**Bonus: legal domain performance.** On BigLaw Bench, Opus 4.6 scores **90.2%** with **40% perfect scores** and 84% above 0.8. On GDPval-AA (economically valuable knowledge work in finance, legal, and other domains), it outperforms GPT-5.2 by **144 Elo points** and its predecessor Opus 4.5 by **190 points**. FlipSide puts that legal reasoning directly in the user's hands.

Plus: **vision** catches formatting tricks text extraction misses, **self-correction** reviews the analysis for false positives before the user sees it, and **split-model parallel** lets Haiku deliver instant cards while Opus builds the expert verdict.

---

## The Meta-Prompting Discovery

During the hackathon, we stumbled on something that even the Claude Code team couldn't fully explain.

When you ask Claude to **"analyze this contract"**, you get a decent analysis. But when you ask Claude to **"write a prompt for analyzing this contract"** and then say **"now execute that prompt"** — the results are dramatically better.

Why? The two-step approach forces the model to separate *planning* from *execution*. In the first step, it reasons about what makes a good analysis. In the second step, it follows its own expert framework. It's chain-of-thought at the meta level.

**Cat Wu** (Product Lead and co-creator of Claude Code) mentioned this pattern during the hackathon AMA — the effect is real, though the exact mechanism isn't fully understood internally.

We validated the pattern by running 30 Opus 4.6 agents (10 tasks × 3 approaches: direct action, plan mode, meta-prompt) and documented 7 real before/after cases from the build. The full analysis — including methodology caveats and an honest limitations section — is in [meta-prompting-strategy.md](meta-prompting-strategy.md).

FlipSide's entire architecture is a **productized version of this discovery**. The system prompt teaches Claude *how to think about documents*: adopt the drafter's perspective, apply a taxonomy of 18 legal trick types (Silent Waiver, Time Trap, Cascade Clause, Phantom Protection, Honey Trap...), contrast "what the small print says" against "what you should read." The prompt is a pre-built reasoning framework that every uploaded document executes against.

The user never sees this meta-prompt. They just see better results.

---

## The Principle

FlipSide applies **"Think Like a Document"** (CHI 2026, Henk van Ess) to a new domain:

| In search | In FlipSide |
| --- | --- |
| Don't search using YOUR words | Don't read using YOUR perspective |
| Think like the document you're looking for | Think like the party who drafted the document |
| The document doesn't know your vocabulary | The contract doesn't serve your interests |
| Match the document's language to find it | Adopt the drafter's perspective to understand it |

The underlying principle is the same: **don't take yourself as the measurement of things. Observe what must be there.**

---

## Architecture

```
                    User uploads document / picks sample / pastes URL
                                      │
                                      ▼
                      Flask extracts text (PDF/DOCX/paste/URL)
                                      │
                              Haiku pre-scan identifies clauses
                                      │
                      ┌───────────────┼───────────────┐
                      │               │               │
                      ▼               ▼               ▼
               ┌─────────────┐ ┌───────────┐ ┌──────────────────┐
               │  HAIKU 4.5  │ │  OPUS 4.6 │ │  SSE stream      │
               │  N parallel │ │  Single   │ │  opens at t=0    │
               │  card       │ │  verdict  │ │                  │
               │  workers    │ │  thread   │ │  Opus events     │
               │             │ │           │ │  flow immediately│
               │  Full cards │ │  One-     │ │                  │
               │  (front +   │ │  screen   │ │  10s auto-reveal │
               │   back)     │ │  verdict  │ │  if no cards yet │
               └──────┬──────┘ └─────┬─────┘ └──────────────────┘
                      │              │
         ┌────────────┤              │
         ▼            ▼              ▼
    First card   Cards trickle   Verdict ready
    at ~8s       in ~8-15s       ~30-60s
                                      │
                              ┌───────┴───────┐
                              │  GO DEEPER    │
                              │  (on demand)  │
                              │               │
                              │  4 specialist │
                              │  Opus calls   │
                              └───────────────┘
```

**Stream-First Pipeline**

| Stage | Agent | What It Does | What the User Sees |
| --- | --- | --- | --- |
| 1. Upload Prescan | Haiku 4.5 | Identifies clauses during upload; card workers launch from results | Investigation screen with real document text + scanning loupe |
| 2. Parallel Cards | Haiku 4.5 × N | N parallel workers generate full flip cards (front + back) | First card appears at ~8s, rest trickle in. Fastest card first. |
| 3. Expert Verdict | Opus 4.6 | One-screen verdict: tier, main risk, power ratio, jurisdiction, checklist | Streams from t=0. Auto-reveals at 10s if cards haven't arrived. |
| 4. Go Deeper | Opus 4.6 (on demand) | 4 specialist deep dives: combinations, power balance, archaeology, counter-draft | User clicks a button → dedicated Opus analysis streams in |
| 5. Follow-up | Opus 4.6 (interactive) | User questions → Opus traces through all clauses | Consultation + message-the-company |

**Pipeline parallelism — the key optimization.** Clause identification runs during upload as a Haiku pre-scan. N parallel Haiku card workers launch from the pre-scan results. Cards emit in completion order (fastest first), not index order. The single Opus verdict thread runs from t=0 in parallel with card generation, so the user is never staring at a blank screen. On-demand "Go Deeper" calls only fire when the user requests them — no wasted compute.

**Key architectural insight:** We originally put Opus 4.6 on the card backs — assuming the flip needed the most powerful model. Haiku does a great job on cards. Opus's real value is in the work Haiku *can't* do: cross-clause reasoning, power analysis, jurisdiction detection, and self-correction. We also started with 4 parallel Opus threads (interactions, asymmetry, archaeology, overall) but consolidated to a single verdict thread that covers everything in one pass — then offers 4 specialist deep dives on demand. Each model does what it does best. See [strategy.md](strategy.md) for the full decision story.

**Tech stack:** Python/Flask, Server-Sent Events, Anthropic API (Haiku 4.5 for cards + Opus 4.6 for verdict with adaptive thinking, vision, prompt caching), single-file HTML/CSS/JS frontend, DOMPurify for XSS protection. 14 built-in sample documents with generated thumbnails (including a real Coca-Cola sweepstakes and the real hackathon event waiver). No external APIs beyond Anthropic. No database required. Deployable behind a reverse proxy with URL prefix.

---

## The Demo Moment

The user picks **"Peace of Mind"** — a homeowner's insurance policy. The first card appears in ~8 seconds.

Card front (the naive reader):

> **"Comprehensive coverage for your peace of mind"**
>
> I'D THINK: OK, so my house is covered for "direct physical loss" — that sounds like everything. Water damage, fire, theft. Good. That's exactly what I'm paying for.

The audience nods. That IS what they'd think.

User flips the card.

Card back (the expert analysis):

> **RED** · Score: 82/100 · Trick: **Phantom Protection**
>
> **$0 payout on a $50,000 water damage claim.** Exclusion 2(c) excludes "gradual seepage over 14 days." Clause 2(e) sets the inspection window at 30 days. Together: virtually all residential water damage can be reclassified as "gradual" after the fact. The broad coverage on the front is a psychological anchor — it makes you stop reading before you reach the exclusions that take it away.

Meanwhile, the Opus verdict has been streaming since t=0 — it's already identified that Clauses 2(c) and 2(e) create a compound trap and auto-detected the jurisdiction.

Then the user clicks **"Message the Company"** — and Opus drafts a professional letter citing the specific clauses, ready to copy or email.

---

## What This Is Not

* Not a legal advice tool (it analyzes documents, it does not give legal recommendations)
* Not a contract generator (it reads existing documents, it does not create new ones)
* Not a diff tool (it reveals strategic intent, not textual differences)
* Not a chatbot (it performs one analysis per document — though you can ask follow-up questions and take action after)

---

## FlipSide vs. Anthropic's Legal Plugin

On February 2, 2026, Anthropic launched a [legal plugin for Claude Cowork](https://legaltechnology.com/2026/02/03/anthropic-unveils-claude-legal-plugin-and-causes-market-meltdown/) — enterprise contract review that crashed legal tech stocks by $50B+. FlipSide approaches the same domain from the opposite direction. The plugin reviews contracts FOR legal teams. FlipSide reveals what the drafting team intended — for the millions of people who sign documents without a legal team. Same model. Different side of the table.

|  | Anthropic Legal Plugin | FlipSide |
| --- | --- | --- |
| **User** | In-house corporate counsel | Anyone who received a document they didn't write |
| **Perspective** | Reviews FROM your configured playbook | Flips TO the drafter's perspective — no playbook needed |
| **Documents** | Contracts your legal team handles | Anything: leases, insurance, ToS, gym memberships, coupon booklets, sweepstakes rules, pet adoption papers |
| **Output** | Redline suggestions, NDA triage | Flip cards, villain voice, YOUR MOVE actions, message-the-company, counter-draft |
| **Requires** | Cowork enterprise license, MCP integrations, configured playbook | Nothing — upload and go. Or pick a sample. |
| **Approach** | Automates what a lawyer already does | Reveals what the lawyer on the other side was thinking |

---

## Problem Statement Fit

**Primary: Break the Barriers**
Legal document analysis is locked behind expertise ($300-500/hour attorneys) and cost. FlipSide puts it in everyone's hands — from a tenant signing a lease to a shopper reading coupon fine print.

**Secondary: Amplify Human Judgment**
FlipSide doesn't replace the user's decision to sign or not sign. It makes them dramatically more informed — human in the loop, but now with the other side's perspective visible.

---

## The Process

This project documents not just the product, but the entire decision-making process — including five documented AI failures and two new methodologies for working with AI:

| Document | What It Covers |
| --- | --- |
| [Hackathon Log](HACKATHON_LOG.md) | 77 entries, complete process timeline |
| [Strategy Decisions](strategy.md) | 26 strategy decisions with rationale — including a midpoint self-evaluation |
| [The Prewash Method](docs/PREWASH_METHOD.md) | How to clean bias from AI prompts before execution |
| [Live Demonstration](docs/LIVE_DEMONSTRATION.md) | "Think Like a Document" demonstrated on the AI itself |
| [Prewash Prompt Collection](docs/PREWASH_PROMPT_COLLECTION.md) | 7 real before/after prompt examples |
| [Five AI Failures](docs/ANCHORING_FAILURE.md) | Confirmation bias, framing bias, vocabulary bias, adjective bias, format rigidity — all caught by the human |
| [All docs](docs/) | 18+ methodology and decision documents |

---

## What's Next

* **Browser extension** — Flag Terms of Service on any website before you click "I Agree." The same pipeline, triggered by a single button on any page with legal text.
* **Collaborative review** — Share your analysis with a lawyer or community group via link.
* **Benchmarking** — Compare your lease or insurance policy against anonymized analyses of similar documents. *"Your landlord's late fee clause is harsher than 82% of leases we've seen."*

---

## Builder

**Henk van Ess** — International OSINT expert, journalist trainer, tool builder. Assessor for IFCN and EFCSN. Early Bellingcat contributor. 20+ years verification methodology. CHI 2026 paper on "Think Like a Document." 10,000+ newsletter subscribers from BBC, NYT, Reuters, Europol, Harvard, MIT, NATO. See [BUILDER_PROFILE.md](BUILDER_PROFILE.md).

## Hackathon

Built with Opus 4.6: a Claude Code Hackathon (February 2026)

---

## Third-Party Licenses

FlipSide is [MIT licensed](LICENSE). It uses the following open-source libraries:

| Library | License | Use |
| --- | --- | --- |
| [Flask](https://flask.palletsprojects.com/) | BSD-3-Clause | Web framework |
| [Anthropic Python SDK](https://github.com/anthropics/anthropic-sdk-python) | MIT | Claude API client |
| [python-docx](https://python-docx.readthedocs.io/) | MIT | DOCX text extraction |
| [pdfplumber](https://github.com/jsvine/pdfplumber) | MIT | PDF text extraction |
| [python-dotenv](https://github.com/theskumar/python-dotenv) | BSD-3-Clause | Environment variables |
| [marked.js](https://marked.js.org/) | MIT | Markdown rendering |
| [DOMPurify](https://github.com/cure53/DOMPurify) | Apache-2.0 / MPL-2.0 | XSS sanitization |
| [DM Sans](https://fonts.google.com/specimen/DM+Sans) | Open Font License | Body typography |
| [JetBrains Mono](https://www.jetbrains.com/lp/mono/) | Open Font License | Monospace labels |

---

## Why This Is Open Source

FlipSide is released under the [MIT License](LICENSE) — free for anyone to use, modify, and distribute.

This project was built entirely with Claude Code by someone who does not write code. Every line — the Flask backend, the SSE streaming architecture, the parallel pipeline, the flip card frontend — was written through conversation with Claude Opus 4.6.

That changes who gets to build software. If a journalist with zero programming experience can ship a 9,500-line frontend and a 3,500-line backend in a weekend, then the barrier to building tools is gone. The only barrier left is having something worth building.

This is in the public domain of ideas because the tools that made it possible should produce things that are accessible to everyone. The people who need FlipSide most — tenants, patients, borrowers, app users, shoppers, pet owners, newlyweds — are the same people who could never afford to hire a developer to build it. Now they don't have to.

---

*FlipSide: the dark side of small print. Built during the Claude Code Hackathon 2026.*
